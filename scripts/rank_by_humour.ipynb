{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humour Rating Generation\n",
    "\n",
    "Run in order unless specified otherwise in Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_COCKAMAMIE_DATA_SRC = \"data/cockamamie_gobbledegook_us_data.json\"\n",
    "_DATAMUSE_DATA_SRC = \"data/ol_gte2.2022-09-26.words\"\n",
    "_EH_DATA_SRC = \"data/humor_dataset.csv\"\n",
    "\n",
    "_VEC_SRC = \"data/crawl-300d-2M-subword.bin\"\n",
    "_PKL_SRC = \"data/crawl-300d-2M-subword.pkl\"\n",
    "\n",
    "_OUTPUT_SRC = \"output.json\"\n",
    "\n",
    "_COMPONENT_KEYS = [\"snd\", \"scatc\", \"clq\", \"inslt\", \"juxt\", \"sexc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **only one** of the two next blocks.\n",
    "\n",
    "Run the 1st block to find humour scores for Cockamamie Gobbledegook words.\n",
    "\n",
    "Run the 2nd block to find humour scores for Datamuse words (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load sample_words from Cockamamie Gobbledegook data.\n",
    "\"\"\"\n",
    "\n",
    "with open(_COCKAMAMIE_DATA_SRC, \"r\") as f:\n",
    "    user_data = json.load(f)\n",
    "    votes = user_data[\"word_ratings\"][\"votes\"]\n",
    "\n",
    "# 0th index for all 128k words whose funniness were voted on by humans\n",
    "# 1st index for the 8k funniest words whose funniness were voted on by humans\n",
    "# 2nd index for the 216 funniest words whose funiness were voted on by humans\n",
    "# 3rd and 4th indices unknown\n",
    "sample_words = [sorted(ws) for ws in votes][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load sample_words from Datamuse data.\n",
    "\"\"\"\n",
    "\n",
    "with open(_DATAMUSE_DATA_SRC, \"r\") as f:\n",
    "    sample_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load training data for overall score from the EH dataset.\n",
    "\"\"\"\n",
    "\n",
    "with open(_EH_DATA_SRC, \"r\") as f:\n",
    "    cells = [line.strip().split(\",\") for line in f.readlines()]\n",
    "    headings = cells[0][1:]\n",
    "    entries = {\n",
    "        row[0]: {feat: float(v) for v, feat in zip(row[1:], headings)}\n",
    "        for row in cells[1:]\n",
    "    }\n",
    "\n",
    "overall_training_data = {w: entries[w][\"mean\"] for w in entries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load training data for component scores from the Cockamamie Gobbledegook dataset.\n",
    "\"\"\"\n",
    "\n",
    "with open(_COCKAMAMIE_DATA_SRC, \"r\") as f:\n",
    "    component_training_data = json.load(f)[\"word_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_training_words = list(overall_training_data)\n",
    "component_training_words = list(component_training_data[\"snd\"])  # contains the same words for all keys in component_training_data\n",
    "\n",
    "testing_words = overall_training_words + component_training_words + sample_words  # combine all seen words\n",
    "testing_words = list(set(testing_words))  # remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the \"Pickle the `.vec` file\" section to create/update the `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict the humour score of all words.\n",
    "Outputs (word, humour score) pairs (.json) and ordered list (by humour) (.txt).\n",
    "\"\"\"\n",
    "\n",
    "def append_predictions_to_output_data(acc, output_key, training_data):\n",
    "    \"\"\"\n",
    "    ``acc``: The \"running sum\" of all data given by append_to_output_data (similar to the accumulator in tail recursion).\n",
    "    ``output_key``: the key in ``acc`` to store the predictions made (for this call).\n",
    "    ``training_data``: The data used to fit the linear regression model.\n",
    "    \"\"\"\n",
    "    with open(_PKL_SRC, \"rb\") as f:\n",
    "        E = pkl.load(f)\n",
    "\n",
    "    clr = LinearRegression()\n",
    "    clr.fit([E[w] for w in training_data], [training_data[w] for w in training_data])\n",
    "    predictions = {\n",
    "        w: float(v)\n",
    "        for (w, v) in zip(testing_words, clr.predict([E[w] for w in testing_words]))\n",
    "    }\n",
    "\n",
    "    for word, score in predictions.items():\n",
    "        acc[word][output_key] = score\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def export_output_data(data):\n",
    "    with open(_OUTPUT_SRC, \"w\") as f:\n",
    "        print(json.dumps(data), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = defaultdict(lambda: dict())\n",
    "\n",
    "output_data = append_predictions_to_output_data(output_data, \"overall\", overall_training_data)\n",
    "for key in _COMPONENT_KEYS:\n",
    "    output_data = append_predictions_to_output_data(output_data, key, component_training_data[key])\n",
    "\n",
    "export_output_data(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the `.vec` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_file2dict(vec_filename, pkl_filename):\n",
    "\n",
    "    model = fasttext.load_model(vec_filename)\n",
    "\n",
    "    def get_word(w):\n",
    "        try:\n",
    "            v = model[w.replace(\"_\", \" \")]\n",
    "            return v / np.linalg.norm(v)  # normalize v\n",
    "        except:\n",
    "            return 0.0 * model[\"cat\"]\n",
    "\n",
    "    e_dict = {w: get_word(w) for w in testing_words}\n",
    "    with open(pkl_filename, \"wb\") as f:\n",
    "        pkl.dump(e_dict, f)\n",
    "\n",
    "    non_zero_vectors = [v for v in e_dict.values() if not np.allclose(v, 0)]\n",
    "    print(\n",
    "        f'Loaded \"{vec_filename}\" and wrote {len(non_zero_vectors):,} non-zero vectors to \"{pkl_filename}\"'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_file2dict(_VEC_SRC, _PKL_SRC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('funnybone-RpkPioHw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab0c6f42f159c804ef77236ccc3ebc8649ab7b351fdb1bd444bc2aab63a609e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
